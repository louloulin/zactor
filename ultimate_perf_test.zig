const std = @import("std");
const Thread = std.Thread;

// Import the ultra high performance system
const UltraHighPerfSystem = @import("src/ultra_high_perf_system.zig").UltraHighPerfSystem;
const SystemConfig = @import("src/ultra_high_perf_system.zig").SystemConfig;
const BatchMessageData = @import("src/ultra_high_perf_system.zig").BatchMessageData;
const FastMessage = @import("src/message_pool.zig").FastMessage;

// Import example actors
const HighPerfCounterActor = @import("src/example_actors.zig").HighPerfCounterActor;
const HighPerfAggregatorActor = @import("src/example_actors.zig").HighPerfAggregatorActor;
const HighPerfBatchProcessorActor = @import("src/example_actors.zig").HighPerfBatchProcessorActor;

// ç»ˆææ€§èƒ½æµ‹è¯• - ç›®æ ‡: 100M+ msg/s
pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    std.log.info("ğŸš€ === ç»ˆææ€§èƒ½æµ‹è¯• - ç›®æ ‡: 100M+ MSG/S ===", .{});

    // æµ‹è¯•é…ç½®
    const test_configs = [_]struct {
        name: []const u8,
        actors: u32,
        worker_threads: u32,
        test_duration_s: u32,
        batch_size: u32,
        target_throughput: u64, // msg/s
    }{
        .{ .name = "çƒ­èº«æµ‹è¯•", .actors = 4, .worker_threads = 4, .test_duration_s = 5, .batch_size = 100, .target_throughput = 1000000 },
        .{ .name = "ä¸­ç­‰è´Ÿè½½", .actors = 16, .worker_threads = 8, .test_duration_s = 10, .batch_size = 500, .target_throughput = 10000000 },
        .{ .name = "é«˜è´Ÿè½½", .actors = 64, .worker_threads = 16, .test_duration_s = 15, .batch_size = 1000, .target_throughput = 50000000 },
        .{ .name = "æé™è´Ÿè½½", .actors = 256, .worker_threads = 32, .test_duration_s = 20, .batch_size = 1000, .target_throughput = 100000000 },
    };

    for (test_configs) |config| {
        std.log.info("\nğŸ§ª === {s} ===", .{config.name});
        std.log.info("é…ç½®: {} actors, {} workers, {}s, batch={}, ç›®æ ‡={}M msg/s", .{ config.actors, config.worker_threads, config.test_duration_s, config.batch_size, config.target_throughput / 1000000 });

        try runPerformanceTest(allocator, config);

        // æµ‹è¯•é—´éš”
        std.time.sleep(2000 * std.time.ns_per_ms);
    }

    std.log.info("\nğŸ† === ç»ˆææ€§èƒ½æµ‹è¯•å®Œæˆ ===", .{});
}

fn runPerformanceTest(
    allocator: std.mem.Allocator,
    config: anytype,
) !void {
    // åˆ›å»ºè¶…é«˜æ€§èƒ½ç³»ç»Ÿ
    const system_config = SystemConfig{
        .worker_threads = config.worker_threads,
        .enable_work_stealing = true,
        .high_priority_ratio = 0.1,
    };

    var system = try UltraHighPerfSystem.init(allocator, system_config);
    defer system.deinit();

    // å¯åŠ¨ç³»ç»Ÿ
    try system.start();
    defer system.stop();

    // åˆ›å»ºä¸åŒç±»å‹çš„Actor
    var actors = std.ArrayList(@import("src/ultra_high_perf_system.zig").ActorRef).init(allocator);
    defer actors.deinit();

    // åˆ›å»ºè®¡æ•°å™¨Actor (50%)
    const counter_count = config.actors / 2;
    for (0..counter_count) |i| {
        const name = try std.fmt.allocPrint(allocator, "Counter-{}", .{i});
        defer allocator.free(name);

        const counter = HighPerfCounterActor.init(name);
        const actor_ref = try system.spawn(HighPerfCounterActor, counter, name);
        try actors.append(actor_ref);
    }

    // åˆ›å»ºèšåˆå™¨Actor (25%)
    const aggregator_count = config.actors / 4;
    for (0..aggregator_count) |i| {
        const name = try std.fmt.allocPrint(allocator, "Aggregator-{}", .{i});
        defer allocator.free(name);

        const aggregator = HighPerfAggregatorActor.init(name);
        const actor_ref = try system.spawn(HighPerfAggregatorActor, aggregator, name);
        try actors.append(actor_ref);
    }

    // åˆ›å»ºæ‰¹å¤„ç†Actor (25%)
    const batch_count = config.actors - counter_count - aggregator_count;
    for (0..batch_count) |i| {
        const name = try std.fmt.allocPrint(allocator, "BatchProcessor-{}", .{i});
        defer allocator.free(name);

        const batch_processor = HighPerfBatchProcessorActor.init(name);
        const actor_ref = try system.spawn(HighPerfBatchProcessorActor, batch_processor, name);
        try actors.append(actor_ref);
    }

    std.log.info("âœ… åˆ›å»ºäº† {} ä¸ªActor", .{actors.items.len});

    // ç­‰å¾…Actorå¯åŠ¨
    std.time.sleep(100 * std.time.ns_per_ms);

    // å¯åŠ¨å¤šä¸ªç”Ÿäº§è€…çº¿ç¨‹
    const producer_count = @min(config.worker_threads, 16);
    var producer_threads: [16]Thread = undefined;
    var producer_stats: [16]ProducerStats = undefined;

    var running = std.atomic.Value(bool).init(true);

    std.log.info("ğŸš€ å¯åŠ¨ {} ä¸ªç”Ÿäº§è€…çº¿ç¨‹", .{producer_count});

    // å¯åŠ¨ç”Ÿäº§è€…çº¿ç¨‹
    for (0..producer_count) |i| {
        producer_stats[i] = ProducerStats.init();
        producer_threads[i] = try Thread.spawn(.{}, producerWorker, .{ i, actors.items, &running, &producer_stats[i], config.batch_size });
    }

    const test_start = std.time.nanoTimestamp();
    std.log.info("ğŸ“Š å¼€å§‹ {} ç§’æ€§èƒ½æµ‹è¯•...", .{config.test_duration_s});

    // è¿è¡ŒæŒ‡å®šæ—¶é—´
    std.time.sleep(@as(u64, config.test_duration_s) * std.time.ns_per_s);

    // åœæ­¢ç”Ÿäº§è€…
    running.store(false, .release);
    std.log.info("ğŸ›‘ åœæ­¢ç”Ÿäº§è€…çº¿ç¨‹...", .{});

    // ç­‰å¾…ç”Ÿäº§è€…çº¿ç¨‹ç»“æŸ
    for (producer_threads[0..producer_count]) |thread| {
        thread.join();
    }

    // ç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæˆ
    std.time.sleep(1000 * std.time.ns_per_ms);

    const test_end = std.time.nanoTimestamp();
    const total_time_ms = @divTrunc(test_end - test_start, 1000000);

    // æ”¶é›†ç»Ÿè®¡
    var total_sent: u64 = 0;
    for (producer_stats[0..producer_count]) |stats| {
        total_sent += stats.messages_sent.load(.monotonic);
    }

    // å‘é€pingè·å–æœ€ç»ˆç»Ÿè®¡
    for (actors.items) |actor| {
        _ = try actor.sendPing();
    }
    std.time.sleep(100 * std.time.ns_per_ms);

    // è·å–ç³»ç»Ÿç»Ÿè®¡
    const system_stats = system.getStats();

    // è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    const send_throughput = @divTrunc(total_sent * 1000, @as(u64, @intCast(total_time_ms)));
    const process_throughput = @divTrunc(system_stats.messages_processed * 1000, @as(u64, @intCast(total_time_ms)));

    std.log.info("\nğŸ“Š === {s} æ€§èƒ½ç»“æœ ===", .{config.name});
    std.log.info("æµ‹è¯•æ—¶é—´: {}ms", .{total_time_ms});
    std.log.info("æ€»å‘é€: {} æ¶ˆæ¯", .{total_sent});
    std.log.info("æ€»å¤„ç†: {} æ¶ˆæ¯", .{system_stats.messages_processed});
    std.log.info("å‘é€ååé‡: {d:.1}M msg/s", .{@as(f64, @floatFromInt(send_throughput)) / 1000000.0});
    std.log.info("å¤„ç†ååé‡: {d:.1}M msg/s", .{@as(f64, @floatFromInt(process_throughput)) / 1000000.0});
    std.log.info("æ¶ˆæ¯å®Œæ•´æ€§: {d:.2}%", .{@as(f64, @floatFromInt(system_stats.messages_processed)) * 100.0 / @as(f64, @floatFromInt(total_sent))});
    std.log.info("å¥åº·Actor: {}/{}", .{ system_stats.healthy_actors, system_stats.total_actors });

    // ç›®æ ‡è¾¾æˆæ£€æŸ¥
    if (process_throughput >= config.target_throughput) {
        std.log.info("ğŸ¯ ç›®æ ‡è¾¾æˆ! å¤„ç†ååé‡ {d:.1}M è¶…è¿‡ç›®æ ‡ {d:.1}M msg/s", .{ @as(f64, @floatFromInt(process_throughput)) / 1000000.0, @as(f64, @floatFromInt(config.target_throughput)) / 1000000.0 });
    } else {
        const percentage = @divTrunc(process_throughput * 100, config.target_throughput);
        std.log.info("ğŸ“ˆ ç›®æ ‡è¿›åº¦: {}% ({d:.1}M / {d:.1}M msg/s)", .{ percentage, @as(f64, @floatFromInt(process_throughput)) / 1000000.0, @as(f64, @floatFromInt(config.target_throughput)) / 1000000.0 });
    }

    // è¯¦ç»†ç»Ÿè®¡
    std.log.info("\nğŸ“ˆ è¯¦ç»†ç»Ÿè®¡:", .{});
    std.log.info("  Actoråˆ›å»º: {}", .{system_stats.actors_created.load(.monotonic)});
    std.log.info("  æ¶ˆæ¯å‘é€: {}", .{system_stats.messages_sent.load(.monotonic)});
    std.log.info("  æ¶ˆæ¯ä¸¢å¼ƒ: {}", .{system_stats.messages_dropped});
    std.log.info("  è°ƒåº¦æ¬¡æ•°: {}", .{system_stats.total_scheduled});
}

// ç”Ÿäº§è€…ç»Ÿè®¡
const ProducerStats = struct {
    messages_sent: std.atomic.Value(u64),

    pub fn init() ProducerStats {
        return ProducerStats{
            .messages_sent = std.atomic.Value(u64).init(0),
        };
    }
};

// ç”Ÿäº§è€…å·¥ä½œçº¿ç¨‹
fn producerWorker(
    thread_id: usize,
    actors: []@import("src/ultra_high_perf_system.zig").ActorRef,
    running: *std.atomic.Value(bool),
    stats: *ProducerStats,
    batch_size: u32,
) void {
    std.log.info("ç”Ÿäº§è€…çº¿ç¨‹ {} å¯åŠ¨", .{thread_id});

    var local_sent: u64 = 0;
    var message_counter: u64 = 0;

    while (running.load(.acquire)) {
        // é€‰æ‹©éšæœºActor
        const actor_idx = std.crypto.random.int(usize) % actors.len;
        const actor = actors[actor_idx];

        // åˆ›å»ºæ‰¹é‡æ¶ˆæ¯
        var batch_messages = std.ArrayList(BatchMessageData).init(std.heap.page_allocator);
        defer batch_messages.deinit();

        for (0..batch_size) |_| {
            message_counter += 1;

            const msg_data = switch (message_counter % 4) {
                0 => BatchMessageData{
                    .msg_type = .user_string,
                    .data = .{ .string = "high_perf_test" },
                },
                1 => BatchMessageData{
                    .msg_type = .user_int,
                    .data = .{ .int_val = @intCast(message_counter) },
                },
                2 => BatchMessageData{
                    .msg_type = .user_float,
                    .data = .{ .float_val = @as(f64, @floatFromInt(message_counter)) * 0.1 },
                },
                3 => BatchMessageData{
                    .msg_type = .system_ping,
                    .data = .{ .int_val = 0 },
                },
                else => unreachable,
            };

            batch_messages.append(msg_data) catch break;
        }

        // å‘é€æ‰¹é‡æ¶ˆæ¯
        const sent = actor.sendBatch(batch_messages.items) catch 0;
        local_sent += sent;

        // å¶å°”è®©å‡ºCPU
        if (local_sent % 10000 == 0) {
            std.time.sleep(10); // 10çº³ç§’
        }
    }

    // æ›´æ–°ç»Ÿè®¡
    _ = stats.messages_sent.fetchAdd(local_sent, .monotonic);

    std.log.info("ç”Ÿäº§è€…çº¿ç¨‹ {} ç»“æŸ: å‘é€ {} æ¶ˆæ¯", .{ thread_id, local_sent });
}
