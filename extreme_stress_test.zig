const std = @import("std");
const Thread = std.Thread;
const Atomic = std.atomic.Value;

// Import the ultra high performance system
const UltraHighPerfSystem = @import("src/ultra_high_perf_system.zig").UltraHighPerfSystem;
const SystemConfig = @import("src/ultra_high_perf_system.zig").SystemConfig;
const HighPerfCounterActor = @import("src/example_actors.zig").HighPerfCounterActor;

// æé™å‹åŠ›æµ‹è¯• - éªŒè¯ç³»ç»Ÿåœ¨æç«¯æ¡ä»¶ä¸‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½
pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    std.log.info("ğŸ”¥ === ZActor æé™å‹åŠ›æµ‹è¯• ===", .{});
    std.log.info("ç›®æ ‡: éªŒè¯ç™¾ä¸‡çº§æ¶ˆæ¯å¤„ç†èƒ½åŠ›", .{});

    // æé™æµ‹è¯•é…ç½®
    const stress_configs = [_]StressTestConfig{
        .{ .name = "çƒ­èº«æµ‹è¯•", .actors = 4, .workers = 4, .producers = 4, .duration_s = 10, .target_mps = 100000, .burst_mode = false },
        .{ .name = "ä¸­ç­‰å‹åŠ›", .actors = 8, .workers = 8, .producers = 8, .duration_s = 15, .target_mps = 500000, .burst_mode = false },
        .{ .name = "é«˜å‹æµ‹è¯•", .actors = 16, .workers = 16, .producers = 12, .duration_s = 20, .target_mps = 1000000, .burst_mode = false },
        .{ .name = "æé™æµ‹è¯•", .actors = 32, .workers = 16, .producers = 16, .duration_s = 30, .target_mps = 2000000, .burst_mode = true },
        .{ .name = "è¶…æé™", .actors = 64, .workers = 16, .producers = 20, .duration_s = 60, .target_mps = 5000000, .burst_mode = true },
    };

    var all_passed = true;
    var total_messages: u64 = 0;
    var total_time: u64 = 0;

    for (stress_configs, 0..) |config, i| {
        std.log.info("\nğŸ”¥ === å‹åŠ›æµ‹è¯• {}: {s} ===", .{ i + 1, config.name });
        std.log.info("é…ç½®: {} actors, {} workers, {} producers, {}s", .{ config.actors, config.workers, config.producers, config.duration_s });
        std.log.info("ç›®æ ‡: {d:.1}M msg/s, çªå‘æ¨¡å¼: {}", .{ @as(f64, @floatFromInt(config.target_mps)) / 1000000.0, config.burst_mode });

        const result = runStressTest(allocator, config) catch |err| {
            std.log.err("âŒ å‹åŠ›æµ‹è¯•å¤±è´¥: {}", .{err});
            all_passed = false;
            break;
        };

        total_messages += result.messages_processed;
        total_time += result.duration_ms;

        if (result.passed) {
            std.log.info("âœ… å‹åŠ›æµ‹è¯•é€šè¿‡: {s}", .{config.name});
            std.log.info("ğŸ“Š å®é™…æ€§èƒ½: {d:.1}M msg/s, å¤„ç†: {}M æ¶ˆæ¯", .{ @as(f64, @floatFromInt(result.throughput_mps)) / 1000000.0, result.messages_processed / 1000000 });
        } else {
            std.log.warn("âŒ å‹åŠ›æµ‹è¯•å¤±è´¥: {s}", .{config.name});
            std.log.warn("ğŸ“Š è¾¾åˆ°æ€§èƒ½: {d:.1}M msg/s (ç›®æ ‡: {d:.1}M)", .{ @as(f64, @floatFromInt(result.throughput_mps)) / 1000000.0, @as(f64, @floatFromInt(config.target_mps)) / 1000000.0 });
            all_passed = false;
            break;
        }

        // æµ‹è¯•é—´éš”ï¼Œè®©ç³»ç»Ÿæ¢å¤
        std.log.info("â³ ç³»ç»Ÿæ¢å¤ä¸­...", .{});
        std.time.sleep(2000 * std.time.ns_per_ms);
    }

    // æœ€ç»ˆç»Ÿè®¡
    std.log.info("\nğŸ† === æé™å‹åŠ›æµ‹è¯•æ€»ç»“ ===", .{});
    if (all_passed) {
        std.log.info("ğŸ¯ æ‰€æœ‰å‹åŠ›æµ‹è¯•é€šè¿‡ï¼ZActorè¾¾åˆ°ä¸–ç•Œçº§æ€§èƒ½ï¼", .{});
    } else {
        std.log.info("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œä½†å·²éªŒè¯é«˜æ€§èƒ½èƒ½åŠ›", .{});
    }

    if (total_time > 0) {
        const overall_throughput = @divTrunc(total_messages * 1000, total_time);
        std.log.info("ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:", .{});
        std.log.info("  - æ€»å¤„ç†æ¶ˆæ¯: {}M", .{total_messages / 1000000});
        std.log.info("  - æ€»æµ‹è¯•æ—¶é—´: {}s", .{total_time / 1000});
        std.log.info("  - å¹³å‡ååé‡: {d:.1}M msg/s", .{@as(f64, @floatFromInt(overall_throughput)) / 1000000.0});
    }
}

const StressTestConfig = struct {
    name: []const u8,
    actors: u32,
    workers: u32,
    producers: u32,
    duration_s: u32,
    target_mps: u64, // messages per second
    burst_mode: bool,
};

const StressTestResult = struct {
    passed: bool,
    messages_processed: u64,
    duration_ms: u64,
    throughput_mps: u64,
    peak_throughput: u64,
    error_rate: f64,
};

fn runStressTest(allocator: std.mem.Allocator, config: StressTestConfig) !StressTestResult {
    // åˆ›å»ºç³»ç»Ÿé…ç½®
    const system_config = SystemConfig{
        .worker_threads = config.workers,
        .enable_work_stealing = true,
        .high_priority_ratio = 0.2,
    };

    var system = try UltraHighPerfSystem.init(allocator, system_config);
    defer system.deinit();

    // å¯åŠ¨ç³»ç»Ÿ
    try system.start();
    defer system.stop();

    // åˆ›å»ºActor
    var actors = std.ArrayList(@import("src/ultra_high_perf_system.zig").ActorRef).init(allocator);
    defer actors.deinit();

    std.log.info("ğŸš€ åˆ›å»º {} ä¸ªActor...", .{config.actors});
    for (0..config.actors) |i| {
        const name = try std.fmt.allocPrint(allocator, "StressActor-{}", .{i});
        defer allocator.free(name);

        const counter = HighPerfCounterActor.init(name);
        const actor_ref = try system.spawn(HighPerfCounterActor, counter, name);
        try actors.append(actor_ref);
    }

    // ç­‰å¾…Actorå¯åŠ¨
    std.time.sleep(200 * std.time.ns_per_ms);

    // åˆ›å»ºç”Ÿäº§è€…ç»Ÿè®¡
    var producer_threads: [32]Thread = undefined;
    var producer_stats: [32]ProducerStats = undefined;
    var running = Atomic(bool).init(true);

    // æ€§èƒ½ç›‘æ§
    var monitor_stats = MonitorStats.init();
    const monitor_thread = try Thread.spawn(.{}, performanceMonitor, .{ &monitor_stats, &running, &system });

    std.log.info("ğŸš€ å¯åŠ¨ {} ä¸ªé«˜å¼ºåº¦ç”Ÿäº§è€…...", .{config.producers});

    // å¯åŠ¨ç”Ÿäº§è€…çº¿ç¨‹
    for (0..config.producers) |i| {
        producer_stats[i] = ProducerStats.init();
        if (config.burst_mode) {
            producer_threads[i] = try Thread.spawn(.{}, burstProducerWorker, .{ i, actors.items, &running, &producer_stats[i] });
        } else {
            producer_threads[i] = try Thread.spawn(.{}, steadyProducerWorker, .{ i, actors.items, &running, &producer_stats[i] });
        }
    }

    const test_start = std.time.nanoTimestamp();
    std.log.info("âš¡ å¼€å§‹ {} ç§’æé™å‹åŠ›æµ‹è¯•...", .{config.duration_s});

    // è¿è¡Œæµ‹è¯•
    std.time.sleep(@as(u64, config.duration_s) * std.time.ns_per_s);

    // åœæ­¢æ‰€æœ‰çº¿ç¨‹
    running.store(false, .release);
    std.log.info("ğŸ›‘ åœæ­¢å‹åŠ›æµ‹è¯•...", .{});

    // ç­‰å¾…ç”Ÿäº§è€…çº¿ç¨‹ç»“æŸ
    for (producer_threads[0..config.producers]) |thread| {
        thread.join();
    }

    // ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
    monitor_thread.join();

    // ç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæˆ
    std.time.sleep(1000 * std.time.ns_per_ms);

    const test_end = std.time.nanoTimestamp();
    const duration_ms: u64 = @intCast(@divTrunc(test_end - test_start, 1000000));

    // æ”¶é›†ç»Ÿè®¡
    var total_sent: u64 = 0;
    var total_errors: u64 = 0;
    for (producer_stats[0..config.producers]) |stats| {
        total_sent += stats.messages_sent.load(.monotonic);
        total_errors += stats.send_errors.load(.monotonic);
    }

    const system_stats = system.getStats();

    // è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    const throughput_mps = @divTrunc(system_stats.messages_processed * 1000, duration_ms);
    const error_rate = if (total_sent > 0)
        @as(f64, @floatFromInt(total_errors)) / @as(f64, @floatFromInt(total_sent))
    else
        0.0;

    // æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
    std.log.info("\nğŸ“Š === å‹åŠ›æµ‹è¯•ç»“æœ ===", .{});
    std.log.info("æµ‹è¯•æ—¶é•¿: {}ms", .{duration_ms});
    std.log.info("å‘é€æ¶ˆæ¯: {}M ({} é”™è¯¯)", .{ total_sent / 1000000, total_errors });
    std.log.info("å¤„ç†æ¶ˆæ¯: {}M", .{system_stats.messages_processed / 1000000});
    std.log.info("å®é™…åå: {d:.1}M msg/s", .{@as(f64, @floatFromInt(throughput_mps)) / 1000000.0});
    std.log.info("å³°å€¼åå: {d:.1}M msg/s", .{@as(f64, @floatFromInt(monitor_stats.peak_throughput.load(.monotonic))) / 1000000.0});
    std.log.info("é”™è¯¯ç‡: {d:.3}%", .{error_rate * 100.0});
    std.log.info("æ¶ˆæ¯å®Œæ•´æ€§: {d:.2}%", .{@as(f64, @floatFromInt(system_stats.messages_processed)) * 100.0 / @as(f64, @floatFromInt(total_sent))});
    std.log.info("å¥åº·Actor: {}/{}", .{ system_stats.healthy_actors, system_stats.total_actors });

    // åˆ¤æ–­æµ‹è¯•æ˜¯å¦é€šè¿‡
    const throughput_ok = throughput_mps >= config.target_mps;
    const integrity_ok = system_stats.messages_processed >= total_sent * 95 / 100; // 95%å®Œæ•´æ€§
    const health_ok = system_stats.healthy_actors == system_stats.total_actors;
    const error_ok = error_rate < 0.01; // 1%ä»¥ä¸‹é”™è¯¯ç‡

    const passed = throughput_ok and integrity_ok and health_ok and error_ok;

    return StressTestResult{
        .passed = passed,
        .messages_processed = system_stats.messages_processed,
        .duration_ms = duration_ms,
        .throughput_mps = throughput_mps,
        .peak_throughput = monitor_stats.peak_throughput.load(.monotonic),
        .error_rate = error_rate,
    };
}

// ç”Ÿäº§è€…ç»Ÿè®¡
const ProducerStats = struct {
    messages_sent: Atomic(u64),
    send_errors: Atomic(u64),

    pub fn init() ProducerStats {
        return ProducerStats{
            .messages_sent = Atomic(u64).init(0),
            .send_errors = Atomic(u64).init(0),
        };
    }
};

// ç›‘æ§ç»Ÿè®¡
const MonitorStats = struct {
    peak_throughput: Atomic(u64),

    pub fn init() MonitorStats {
        return MonitorStats{
            .peak_throughput = Atomic(u64).init(0),
        };
    }
};

// ç¨³å®šç”Ÿäº§è€… - æŒç»­ç¨³å®šå‘é€
fn steadyProducerWorker(
    thread_id: usize,
    actors: []@import("src/ultra_high_perf_system.zig").ActorRef,
    running: *Atomic(bool),
    stats: *ProducerStats,
) void {
    std.log.info("ç¨³å®šç”Ÿäº§è€… {} å¯åŠ¨", .{thread_id});

    var local_sent: u64 = 0;
    var local_errors: u64 = 0;
    var message_counter: u64 = 0;

    while (running.load(.acquire)) {
        // é«˜é¢‘å‘é€æ¶ˆæ¯
        for (0..100) |_| {
            if (!running.load(.acquire)) break;

            const actor_idx = std.crypto.random.int(usize) % actors.len;
            const actor = actors[actor_idx];

            message_counter += 1;
            const sent = switch (message_counter % 3) {
                0 => actor.sendString("stress_test") catch false,
                1 => actor.sendInt(@intCast(message_counter)) catch false,
                2 => actor.sendPing() catch false,
                else => unreachable,
            };

            if (sent) {
                local_sent += 1;
            } else {
                local_errors += 1;
            }
        }

        // å¾®å°å»¶è¿Ÿï¼Œé¿å…CPUè¿‡è½½
        std.time.sleep(10);
    }

    // æ›´æ–°ç»Ÿè®¡
    _ = stats.messages_sent.fetchAdd(local_sent, .monotonic);
    _ = stats.send_errors.fetchAdd(local_errors, .monotonic);

    std.log.info("ç¨³å®šç”Ÿäº§è€… {} ç»“æŸ: å‘é€ {}K, é”™è¯¯ {}", .{ thread_id, local_sent / 1000, local_errors });
}

// çªå‘ç”Ÿäº§è€… - çªå‘é«˜å¼ºåº¦å‘é€
fn burstProducerWorker(
    thread_id: usize,
    actors: []@import("src/ultra_high_perf_system.zig").ActorRef,
    running: *Atomic(bool),
    stats: *ProducerStats,
) void {
    std.log.info("çªå‘ç”Ÿäº§è€… {} å¯åŠ¨", .{thread_id});

    var local_sent: u64 = 0;
    var local_errors: u64 = 0;
    var message_counter: u64 = 0;

    while (running.load(.acquire)) {
        // çªå‘å‘é€ - çŸ­æ—¶é—´å†…å¤§é‡æ¶ˆæ¯
        for (0..1000) |_| {
            if (!running.load(.acquire)) break;

            const actor_idx = std.crypto.random.int(usize) % actors.len;
            const actor = actors[actor_idx];

            message_counter += 1;
            const sent = switch (message_counter % 3) {
                0 => actor.sendString("burst_test") catch false,
                1 => actor.sendInt(@intCast(message_counter)) catch false,
                2 => actor.sendPing() catch false,
                else => unreachable,
            };

            if (sent) {
                local_sent += 1;
            } else {
                local_errors += 1;
            }
        }

        // çŸ­æš‚ä¼‘æ¯åç»§ç»­çªå‘
        std.time.sleep(1000); // 1å¾®ç§’
    }

    // æ›´æ–°ç»Ÿè®¡
    _ = stats.messages_sent.fetchAdd(local_sent, .monotonic);
    _ = stats.send_errors.fetchAdd(local_errors, .monotonic);

    std.log.info("çªå‘ç”Ÿäº§è€… {} ç»“æŸ: å‘é€ {}K, é”™è¯¯ {}", .{ thread_id, local_sent / 1000, local_errors });
}

// æ€§èƒ½ç›‘æ§çº¿ç¨‹
fn performanceMonitor(
    monitor_stats: *MonitorStats,
    running: *Atomic(bool),
    system: *UltraHighPerfSystem,
) void {
    std.log.info("æ€§èƒ½ç›‘æ§å¯åŠ¨", .{});

    var last_processed: u64 = 0;
    var last_time = std.time.nanoTimestamp();

    while (running.load(.acquire)) {
        std.time.sleep(1000 * std.time.ns_per_ms); // æ¯ç§’ç›‘æ§

        const current_stats = system.getStats();
        const current_time = std.time.nanoTimestamp();

        const processed_delta = current_stats.messages_processed - last_processed;
        const time_delta_ms: u64 = @intCast(@divTrunc(current_time - last_time, 1000000));

        if (time_delta_ms > 0) {
            const current_throughput: u64 = @intCast(@divTrunc(processed_delta * 1000, time_delta_ms));

            // æ›´æ–°å³°å€¼ååé‡
            const current_peak = monitor_stats.peak_throughput.load(.monotonic);
            if (current_throughput > current_peak) {
                monitor_stats.peak_throughput.store(current_throughput, .monotonic);
            }

            std.log.info("ğŸ“ˆ å®æ—¶åå: {d:.1}M msg/s, æ€»å¤„ç†: {}M", .{ @as(f64, @floatFromInt(current_throughput)) / 1000000.0, current_stats.messages_processed / 1000000 });
        }

        last_processed = current_stats.messages_processed;
        last_time = current_time;
    }

    std.log.info("æ€§èƒ½ç›‘æ§ç»“æŸ", .{});
}
